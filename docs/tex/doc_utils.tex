\section{\texorpdfstring{\texttt{fitr.utils}}{fitr.utils}}\label{fitr.utils}

Functions used across \texttt{fitr}.

\subsection{batch\_softmax}\label{batch_softmax}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.utils.batch_softmax(X, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Computes the softmax function for a batch of samples

\[
p(\mathbf{x}) = \frac{e^{\mathbf{x} - \max_i x_i}}{\mathbf{1}^\top e^{\mathbf{x} - \max_i x_i}}
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: Softmax logits (\texttt{ndarray((nsamples,nfeatures))})
\end{itemize}

Returns:

Matrix of probabilities of size \texttt{ndarray((nsamples,nfeatures))}
such that sum over \texttt{nfeatures} is 1.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{batch\_transform}\label{batch_transform}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.utils.batch_transform(X, f_list)}
\end{Highlighting}
\end{Shaded}

Applies the \texttt{fitr.utils.transform} function over a batch of
parameters

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{X}: \texttt{ndarray((nsamples,\ nparams))}. Raw parameters
\item
  \textbf{f\_list}: \texttt{list} where \texttt{len(list)\ ==\ nparams}.
  Functions defining coordinate transformations on each element of
  \texttt{x}.
\end{itemize}

Returns:

\texttt{ndarray((nsamples,\ nparams))}. Transformed parameters

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{I}\label{i}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.utils.I(x)}
\end{Highlighting}
\end{Shaded}

Identity transformation.

Mainly for convenience when using \texttt{fitr.utils.transform} with
some vector element that should not be transformed, despite changing the
coordinates of other variables.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray}
\end{itemize}

Returns:

\texttt{ndarray(shape=x.shape)}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{log\_loss}\label{log_loss}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.utils.log_loss(p, q)}
\end{Highlighting}
\end{Shaded}

Computes log loss.

\[
\mathcal L = - \frac{1}{n_s} \big( \mathbf p^\top \log \mathbf q + (1-\mathbf p)^\top \log (1 - \mathbf q) \big)
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{p}: Binary vector of true labels \texttt{ndarray((nsamples,))}
\item
  \textbf{q}: Vector of estimates (between 0 and 1) of type
  \texttt{ndarray((nsamples,))}
\end{itemize}

Returns:

Scalar log loss

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{logsumexp}\label{logsumexp}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.utils.logsumexp(x)}
\end{Highlighting}
\end{Shaded}

Numerically stable logsumexp.

Computed as follows:

\[
\max x + \log \sum_x e^{x - \max x}
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: `ndarray(shape=(nactions,))``
\end{itemize}

Returns:

\texttt{float}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{rank\_data}\label{rank_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.utils.rank_data(x)}
\end{Highlighting}
\end{Shaded}

Ranks a set of observations, assigning the average of ranks to ties.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray(nsamples)}. Vector of data to be compared
\end{itemize}

Returns:

\begin{itemize}
\tightlist
\item
  \textbf{ranks}: \texttt{ndarray(nsamples)}. Ranks for each observation
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{rank\_grouped\_data}\label{rank_grouped_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.utils.rank_grouped_data(x, g)}
\end{Highlighting}
\end{Shaded}

Ranks observations taken across several groups

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray(nsamples)}. Vector of data to be compared
\item
  \textbf{g}: \texttt{ndarray(nsamples)}. Group ID's
\end{itemize}

Returns:

\begin{itemize}
\tightlist
\item
  \textbf{ranks}: \texttt{ndarray(nsamples)}. Ranks for each observation
\item
  \textbf{G}: \texttt{ndarray(nsamples,\ ngroups)}. Matrix indicating
  whether sample i is in group j
\item
  \textbf{R}: \texttt{ndarray((nsamples,\ ngroups))}. Matrix indicating
  the rank for sample i in group j
\item
  \textbf{lab}: \texttt{ndarray(ngroups)}. Group labels
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{reduce\_then\_tile}\label{reduce_then_tile}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.utils.reduce_then_tile(X, f, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Computes some reduction function over an axis, then tiles that vector to
create matrix of original size

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{X}: \texttt{ndarray((n,\ m))}. Matrix.
\item
  \textbf{f}: \texttt{function} that reduces data across some axis (e.g.
  \texttt{np.sum()}, \texttt{np.max()})
\item
  \textbf{axis}: \texttt{int} which axis the data should be reduced over
  (only goes over 2 axes for now)
\end{itemize}

Returns:res

\texttt{ndarray((n,\ m))}

Examples:

Here is one way to compute a softmax function over the columns of
\texttt{X}, for each row.

\begin{verbatim}
import numpy as np
X = np.random.normal(0, 1, size=(10, 3))**2
max_x = reduce_then_tile(X, np.max, axis=1)
exp_x = np.exp(X - max_x)
sum_exp_x = reduce_then_tile(exp_x, np.sum, axis=1)
y = exp_x/sum_exp_x
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{relu}\label{relu}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.utils.relu(x, a_max}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Rectified linearity

\[
\mathbf x' = \max (x_i, 0)_{i=1}^{|\mathbf x|}
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: Vector of inputs
\item
  \textbf{a\_max}: Upper bound at which to clip values of \texttt{x}
\end{itemize}

Returns:

Exponentiated values of \texttt{x}.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{scale\_data}\label{scale_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.utils.scale_data(X, axis}\OperatorTok{=}\DecValTok{0}\NormalTok{, with_mean}\OperatorTok{=}\VariableTok{True}\NormalTok{, with_var}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Rescales data by subtracting mean and dividing by variance

\[
\mathbf x' = \frac{\mathbf x - \frac{1}{n} \mathbf 1^\top \mathbf x}{Var(\mathbf x)}
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{X}: \texttt{ndarray((nsamples,\ {[}nfeatures{]}))}. Data. May
  be 1D or 2D.
\item
  \textbf{with\_mean}: \texttt{bool}. Whether to subtract the mean
\item
  \textbf{with\_var}: \texttt{bool}. Whether to normalize for variance
\end{itemize}

Returns:

\texttt{ndarray(X.shape)}. Rescaled data.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{sigmoid}\label{sigmoid}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.utils.sigmoid(x, a_min}\OperatorTok{=-}\DecValTok{10}\NormalTok{, a_max}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Sigmoid function

\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: Vector
\item
  \textbf{a\_min}: Lower bound at which to clip values of \texttt{x}
\item
  \textbf{a\_max}: Upper bound at which to clip values of \texttt{x}
\end{itemize}

Returns:

Vector between 0 and 1 of size \texttt{x.shape}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{softmax}\label{softmax}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.utils.softmax(x)}
\end{Highlighting}
\end{Shaded}

Computes the softmax function

\[
p(\mathbf{x}) = \frac{e^{\mathbf{x} - \max_i x_i}}{\mathbf{1}^\top e^{\mathbf{x} - \max_i x_i}}
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: Softmax logits (\texttt{ndarray((N,))})
\end{itemize}

Returns:

Vector of probabilities of size \texttt{ndarray((N,))}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{stable\_exp}\label{stable_exp}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.utils.stable_exp(x, a_min}\OperatorTok{=-}\DecValTok{10}\NormalTok{, a_max}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Clipped exponential function

Avoids overflow by clipping input values.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: Vector of inputs
\item
  \textbf{a\_min}: Lower bound at which to clip values of \texttt{x}
\item
  \textbf{a\_max}: Upper bound at which to clip values of \texttt{x}
\end{itemize}

Returns:

Exponentiated values of \texttt{x}.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{transform}\label{transform}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.utils.transform(x, f_list)}
\end{Highlighting}
\end{Shaded}

Transforms parameters from domain in \texttt{x} into some new domain
defined by \texttt{f\_list}

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nparams,))}. Parameter vector in some
  domain.
\item
  \textbf{f\_list}: \texttt{list} where \texttt{len(list)\ ==\ nparams}.
  Functions defining coordinate transformations on each element of
  \texttt{x}.
\end{itemize}

Returns:

\begin{itemize}
\tightlist
\item
  \textbf{x\_}: \texttt{ndarray((nparams,))}. Parameter vector in new
  coordinates.
\end{itemize}

Examples:

Applying \texttt{fitr} transforms can be done as follows.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ fitr.utils }\ImportTok{import}\NormalTok{ transform, sigmoid, relu}

\NormalTok{x }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, size}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\NormalTok{x_}\OperatorTok{=}\NormalTok{ transform(x, [sigmoid, relu, relu])}
\end{Highlighting}
\end{Shaded}

You can also apply other functions, so long as dimensions are equal for
input and output.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ fitr.utils }\ImportTok{import}\NormalTok{ transform}

\NormalTok{x  }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, size}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\NormalTok{x_ }\OperatorTok{=}\NormalTok{ transform(x, [np.square, np.sqrt, np.exp])}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}
