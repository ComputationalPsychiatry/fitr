\section{\texorpdfstring{\texttt{fitr.agents}}{fitr.agents}}\label{fitr.agents}

A modular way to build and test reinforcement learning agents.

There are three main submodules:

\begin{itemize}
\tightlist
\item
  \texttt{fitr.agents.policies}: which describe a class of functions
  essentially representing \(f:\mathcal X \to \mathcal U\)
\item
  \texttt{fitr.agents.value\_functions}: which describe a class of
  functions essentially representing
  \(\mathcal V: \mathcal X \to \mathbb R\) and/or
  \(\mathcal Q: \mathcal Q \times \mathcal U \to \mathbb R\)
\item
  \texttt{fitr.agents.agents}: classes of agents that are combinations
  of policies and value functions, along with some convenience functions
  for generating data from \texttt{fitr.environments.Graph}
  environments.
\end{itemize}

\subsection{SoftmaxPolicy}\label{softmaxpolicy}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.SoftmaxPolicy()}
\end{Highlighting}
\end{Shaded}

Action selection by sampling from a multinomial whose parameters are
given by a softmax.

Action sampling is

\[
\mathbf u \sim \mathrm{Multinomial}(1, \mathbf p=\varsigma(\mathbf v)).
\]

Parameters of that distribution are

\[
p(\mathbf u|\mathbf v) = \varsigma(\mathbf v) = \frac{e^{\beta \mathbf v}}{\sum_{i}^{|\mathbf v|} e^{\beta v_i}}.
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{inverse\_softmax\_temp}: Inverse softmax temperature \(\beta\)
\item
  \textbf{rng}: \texttt{np.random.RandomState} object
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SoftmaxPolicy.action\_prob}\label{softmaxpolicy.action_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.action_prob(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Computes the softmax

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SoftmaxPolicy.log\_prob}\label{softmaxpolicy.log_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.log_prob(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Computes the log-probability of an action \(\mathbf u\), in addition to
computing derivatives up to second order

\[
\log p(\mathbf u|\mathbf v) = \beta \mathbf v - \log \sum_{v_i} e^{\beta \mathbf v_i}
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: State vector of type \texttt{ndarray((nstates,))}
\end{itemize}

Returns:

Scalar log-probability

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SoftmaxPolicy.sample}\label{softmaxpolicy.sample}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.sample(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Samples from the action distribution

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{StickySoftmaxPolicy}\label{stickysoftmaxpolicy}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.StickySoftmaxPolicy()}
\end{Highlighting}
\end{Shaded}

Action selection by sampling from a multinomial whose parameters are
given by a softmax, but with accounting for the tendency to perseverate
(i.e.~choosing the previously used action without considering its
value).

Let \(\mathbf u_{t-1} = (u_{t-1}^{(i)})_{i=1}^{|\mathcal U|}\) be a one
hot vector representing the action taken at the last step, and
\(\beta^\rho\) be an inverse softmax temperature for the influence of
this last action.

Action sampling is thus:

\[
\mathbf u \sim \mathrm{Multinomial}(1, \mathbf p=\varsigma(\mathbf v, \mathbf u_{t-1})).
\]

Parameters of that distribution are

\[
p(\mathbf u|\mathbf v, \mathbf u_{t-1}) = \varsigma(\mathbf v, \mathbf u_{t-1}) = \frac{e^{\beta \mathbf v + \beta^\rho \mathbf u_{t-1}}}{\sum_{i}^{|\mathbf v|} e^{\beta v_i + \beta^\rho u_{t-1}^{(i)}}}.
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{inverse\_softmax\_temp}: Inverse softmax temperature \(\beta\)
\item
  \textbf{perseveration}: Inverse softmax temperature \(\beta^\rho\)
  capturing the tendency to repeat the last action taken.
\item
  \textbf{rng}: \texttt{np.random.RandomState} object
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{StickySoftmaxPolicy.action\_prob}\label{stickysoftmaxpolicy.action_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.action_prob(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Computes the softmax

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nactions,))} action value vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} vector of action probabilities

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{StickySoftmaxPolicy.log\_prob}\label{stickysoftmaxpolicy.log_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.log_prob(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Computes the log-probability of an action \(\mathbf u\)

\[
\log p(\mathbf u|\mathbf v, \mathbf u_{t-1}) = \big(\beta \mathbf v + \beta^\rho \mathbf u_{t-1}) - \log \sum_{v_i} e^{\beta \mathbf v_i + \beta^\rho u_{t-1}^{(i)}}
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: State vector of type \texttt{ndarray((nactions,))}
\end{itemize}

Returns:

Scalar log-probability

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{StickySoftmaxPolicy.sample}\label{stickysoftmaxpolicy.sample}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.sample(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Samples from the action distribution

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nactions,))} action value vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} one-hot action vector

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{EpsilonGreedyPolicy}\label{epsilongreedypolicy}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.EpsilonGreedyPolicy()}
\end{Highlighting}
\end{Shaded}

A policy that takes the maximally valued action with probability
\(1-\epsilon\), otherwise chooses randomlyself.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{epsilon}: Probability of not taking the action with highest
  value
\item
  \textbf{rng}: \texttt{numpy.random.RandomState} object
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{EpsilonGreedyPolicy.action\_prob}\label{epsilongreedypolicy.action_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.action_prob(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Creates vector of action probabilities for e-greedy policy

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nstates,))} vector of action probabilities

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{EpsilonGreedyPolicy.sample}\label{epsilongreedypolicy.sample}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.sample(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Samples from the action distribution

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nstates,))} one-hot action vector

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{ValueFunction}\label{valuefunction}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.ValueFunction()}
\end{Highlighting}
\end{Shaded}

A general value function object.

A value function here is task specific and consists of several
attributes:

\begin{itemize}
\tightlist
\item
  \texttt{nstates}: The number of states in the task, \(|\mathcal X|\)
\item
  \texttt{nactions}: Number of actions in the task, \(|\mathcal U|\)
\item
  \texttt{V}: State value function \(\mathbf v = \mathcal V(\mathbf x)\)
\item
  \texttt{Q}: State-action value function
  \(\mathbf Q = \mathcal Q(\mathbf x, \mathbf u)\)
\item
  \texttt{rpe}: Reward prediction error history
\item
  \texttt{etrace}: An eligibility trace (optional)
\item
  \texttt{dV}: A dictionary storing gradients with respect to parameters
  (named keys)
\item
  \texttt{dQ}: A dictionary storing gradients with respect to parameters
  (named keys)
\end{itemize}

Note that in general we rely on matrix-vector notation for value
functions, rather than function notation. Vectors in the mathematical
typesetting are by default column vectors.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{env}: A \texttt{fitr.environments.Graph}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{ValueFunction.Qmax}\label{valuefunction.qmax}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qmax(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Return maximal action value for given state

\[
\max_{u_i}\mathcal Q(\mathbf x, u_i) = \max_{\mathbf u'} \mathbf u'^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of the maximal action value at the given state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{ValueFunction.Qmean}\label{valuefunction.qmean}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qmean(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Return mean action value for given state

\[
Mean \big(\mathcal Q(\mathbf x, :)\big) = \frac{1}{|\mathcal U|} \mathbf 1^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of the maximal action value at the given state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{ValueFunction.Qx}\label{valuefunction.qx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute action values for a given state

\[
\mathcal Q(\mathbf x, :) = \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} vector of values for actions in the given
state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{ValueFunction.Vx}\label{valuefunction.vx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Vx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute value of state \(\mathbf x\)

\[
\mathcal V(\mathbf x) = \mathbf v^\top \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{ValueFunction.grad\_Qx}\label{valuefunction.grad_qx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.grad_Qx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute gradient of action values for a given state

\[
\mathcal Q(\mathbf x, :) = \mathbf Q \mathbf x,
\]

where the gradient is defined as

\[
\frac{\partial}{\partial \mathbf Q} \mathcal Q(\mathbf x, :) = \mathbf 1 \mathbf x^\top,
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} vector of values for actions in the given
state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{ValueFunction.grad\_Vx}\label{valuefunction.grad_vx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.grad_Vx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute the gradient of state value function with respect to parameters
\(\mathbf v\)

\[
\mathcal V(\mathbf x) = \mathbf v^\top \mathbf x,
\]

where the gradient is defined as

\[
\nabla_{\mathbf v} \mathcal V(\mathbf x) = \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{ValueFunction.grad\_uQx}\label{valuefunction.grad_uqx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.grad_uQx(}\VariableTok{self}\NormalTok{, u, x)}
\end{Highlighting}
\end{Shaded}

Compute derivative of value of taking action \(\mathbf u\) in state
\(\mathbf x\) with respect to value function parameters \(\mathbf Q\)

\[
\mathcal Q(\mathbf x, \mathbf u) = \mathbf u^\top \mathbf Q \mathbf x,
\]

where the derivative is defined as

\[
\frac{\partial}{\partial \mathbf Q} \mathcal Q(\mathbf x, \mathbf u) = \mathbf u \mathbf x^\top,
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of action \(\mathbf u\) in state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{ValueFunction.uQx}\label{valuefunction.uqx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.uQx(}\VariableTok{self}\NormalTok{, u, x)}
\end{Highlighting}
\end{Shaded}

Compute value of taking action \(\mathbf u\) in state \(\mathbf x\)

\[
\mathcal Q(\mathbf x, \mathbf u) = \mathbf u^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of action \(\mathbf u\) in state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{ValueFunction.update}\label{valuefunction.update}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.update(}\VariableTok{self}\NormalTok{, x, u, r, x_, u_)}
\end{Highlighting}
\end{Shaded}

Updates the value function

In the context of the base \texttt{ValueFunction} class, this is merely
a placeholder. The specific update rule will depend on the specific
value function desired.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{r}: Scalar reward
\item
  \textbf{x\_}: \texttt{ndarray((nstates,))} one-hot next-state vector
\item
  \textbf{u\_}: \texttt{ndarray((nactions,))} one-hot next-action vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{DummyLearner}\label{dummylearner}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.DummyLearner()}
\end{Highlighting}
\end{Shaded}

A critic/value function for the random learner

This class actually contributes nothing except identifying that a value
function has been chosen for an \texttt{Agent} object

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{env}: A \texttt{fitr.environments.Graph}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{DummyLearner.Qmax}\label{dummylearner.qmax}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qmax(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Return maximal action value for given state

\[
\max_{u_i}\mathcal Q(\mathbf x, u_i) = \max_{\mathbf u'} \mathbf u'^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of the maximal action value at the given state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{DummyLearner.Qmean}\label{dummylearner.qmean}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qmean(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Return mean action value for given state

\[
Mean \big(\mathcal Q(\mathbf x, :)\big) = \frac{1}{|\mathcal U|} \mathbf 1^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of the maximal action value at the given state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{DummyLearner.Qx}\label{dummylearner.qx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute action values for a given state

\[
\mathcal Q(\mathbf x, :) = \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} vector of values for actions in the given
state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{DummyLearner.Vx}\label{dummylearner.vx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Vx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute value of state \(\mathbf x\)

\[
\mathcal V(\mathbf x) = \mathbf v^\top \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{DummyLearner.grad\_Qx}\label{dummylearner.grad_qx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.grad_Qx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute gradient of action values for a given state

\[
\mathcal Q(\mathbf x, :) = \mathbf Q \mathbf x,
\]

where the gradient is defined as

\[
\frac{\partial}{\partial \mathbf Q} \mathcal Q(\mathbf x, :) = \mathbf 1 \mathbf x^\top,
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} vector of values for actions in the given
state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{DummyLearner.grad\_Vx}\label{dummylearner.grad_vx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.grad_Vx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute the gradient of state value function with respect to parameters
\(\mathbf v\)

\[
\mathcal V(\mathbf x) = \mathbf v^\top \mathbf x,
\]

where the gradient is defined as

\[
\nabla_{\mathbf v} \mathcal V(\mathbf x) = \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{DummyLearner.grad\_uQx}\label{dummylearner.grad_uqx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.grad_uQx(}\VariableTok{self}\NormalTok{, u, x)}
\end{Highlighting}
\end{Shaded}

Compute derivative of value of taking action \(\mathbf u\) in state
\(\mathbf x\) with respect to value function parameters \(\mathbf Q\)

\[
\mathcal Q(\mathbf x, \mathbf u) = \mathbf u^\top \mathbf Q \mathbf x,
\]

where the derivative is defined as

\[
\frac{\partial}{\partial \mathbf Q} \mathcal Q(\mathbf x, \mathbf u) = \mathbf u \mathbf x^\top,
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of action \(\mathbf u\) in state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{DummyLearner.uQx}\label{dummylearner.uqx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.uQx(}\VariableTok{self}\NormalTok{, u, x)}
\end{Highlighting}
\end{Shaded}

Compute value of taking action \(\mathbf u\) in state \(\mathbf x\)

\[
\mathcal Q(\mathbf x, \mathbf u) = \mathbf u^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of action \(\mathbf u\) in state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{DummyLearner.update}\label{dummylearner.update}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.update(}\VariableTok{self}\NormalTok{, x, u, r, x_, u_)}
\end{Highlighting}
\end{Shaded}

Updates the value function

In the context of the base \texttt{ValueFunction} class, this is merely
a placeholder. The specific update rule will depend on the specific
value function desired.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{r}: Scalar reward
\item
  \textbf{x\_}: \texttt{ndarray((nstates,))} one-hot next-state vector
\item
  \textbf{u\_}: \texttt{ndarray((nactions,))} one-hot next-action vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{InstrumentalRescorlaWagnerLearner}\label{instrumentalrescorlawagnerlearner}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.InstrumentalRescorlaWagnerLearner()}
\end{Highlighting}
\end{Shaded}

Learns an instrumental control policy through one-step error-driven
updates of the state-action value function

The instrumental Rescorla-Wagner rule is as follows:

\[
\mathbf Q \gets \mathbf Q + \alpha \big(r - \mathbf u^\top \mathbf Q \mathbf x \big) \mathbf u \mathbf x^\top,
\]

where \(0 < \alpha < 1\) is the learning rate, and where the reward
prediction error (RPE) is
\(\delta = (r - \mathbf u^\top \mathbf Q \mathbf x)\).

\$\$

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{env}: A \texttt{fitr.environments.Graph}
\item
  \textbf{learning\_rate}: Learning rate \(\alpha\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{InstrumentalRescorlaWagnerLearner.Qmax}\label{instrumentalrescorlawagnerlearner.qmax}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qmax(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Return maximal action value for given state

\[
\max_{u_i}\mathcal Q(\mathbf x, u_i) = \max_{\mathbf u'} \mathbf u'^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of the maximal action value at the given state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{InstrumentalRescorlaWagnerLearner.Qmean}\label{instrumentalrescorlawagnerlearner.qmean}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qmean(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Return mean action value for given state

\[
Mean \big(\mathcal Q(\mathbf x, :)\big) = \frac{1}{|\mathcal U|} \mathbf 1^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of the maximal action value at the given state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{InstrumentalRescorlaWagnerLearner.Qx}\label{instrumentalrescorlawagnerlearner.qx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute action values for a given state

\[
\mathcal Q(\mathbf x, :) = \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} vector of values for actions in the given
state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{InstrumentalRescorlaWagnerLearner.Vx}\label{instrumentalrescorlawagnerlearner.vx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Vx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute value of state \(\mathbf x\)

\[
\mathcal V(\mathbf x) = \mathbf v^\top \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{InstrumentalRescorlaWagnerLearner.grad\_Qx}\label{instrumentalrescorlawagnerlearner.grad_qx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.grad_Qx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute gradient of action values for a given state

\[
\mathcal Q(\mathbf x, :) = \mathbf Q \mathbf x,
\]

where the gradient is defined as

\[
\frac{\partial}{\partial \mathbf Q} \mathcal Q(\mathbf x, :) = \mathbf 1 \mathbf x^\top,
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} vector of values for actions in the given
state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{InstrumentalRescorlaWagnerLearner.grad\_Vx}\label{instrumentalrescorlawagnerlearner.grad_vx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.grad_Vx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute the gradient of state value function with respect to parameters
\(\mathbf v\)

\[
\mathcal V(\mathbf x) = \mathbf v^\top \mathbf x,
\]

where the gradient is defined as

\[
\nabla_{\mathbf v} \mathcal V(\mathbf x) = \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{InstrumentalRescorlaWagnerLearner.grad\_uQx}\label{instrumentalrescorlawagnerlearner.grad_uqx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.grad_uQx(}\VariableTok{self}\NormalTok{, u, x)}
\end{Highlighting}
\end{Shaded}

Compute derivative of value of taking action \(\mathbf u\) in state
\(\mathbf x\) with respect to value function parameters \(\mathbf Q\)

\[
\mathcal Q(\mathbf x, \mathbf u) = \mathbf u^\top \mathbf Q \mathbf x,
\]

where the derivative is defined as

\[
\frac{\partial}{\partial \mathbf Q} \mathcal Q(\mathbf x, \mathbf u) = \mathbf u \mathbf x^\top,
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of action \(\mathbf u\) in state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{InstrumentalRescorlaWagnerLearner.uQx}\label{instrumentalrescorlawagnerlearner.uqx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.uQx(}\VariableTok{self}\NormalTok{, u, x)}
\end{Highlighting}
\end{Shaded}

Compute value of taking action \(\mathbf u\) in state \(\mathbf x\)

\[
\mathcal Q(\mathbf x, \mathbf u) = \mathbf u^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of action \(\mathbf u\) in state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{InstrumentalRescorlaWagnerLearner.update}\label{instrumentalrescorlawagnerlearner.update}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.update(}\VariableTok{self}\NormalTok{, x, u, r, x_, u_)}
\end{Highlighting}
\end{Shaded}

Computes the value function update of the instrumental Rescorla-Wagner
learning rule and computes derivative with respect to the learning rate.

This derivative is defined as

\[
\frac{\partial}{\partial \alpha} \mathcal Q(\mathbf x, \mathbf u; \alpha) = \delta \mathbf u \mathbf x^\top + \frac{\partial}{\partial \alpha} \mathcal Q(\mathbf x, \mathbf u; \alpha) (1-\alpha \mathbf u \mathbf x^\top)
\]

and the second order derivative with respect to learning rate is

\[
\frac{\partial}{\partial \alpha} \mathcal Q(\mathbf x, \mathbf u; \alpha) = - 2 \mathbf u \mathbf x^\top \partial_\alpha \mathcal Q(\mathbf x, \mathbf u; \alpha) + \partial^2_\alpha \mathcal Q(\mathbf x, \mathbf u; \alpha) (1 - \alpha \mathbf u \mathbf x^\top)
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,\ ))}. State vector
\item
  \textbf{u}: \texttt{ndarray((nactions,\ ))}. Action vector
\item
  \textbf{r}: \texttt{float}. Reward received
\item
  \textbf{x\_}: \texttt{ndarray((nstates,\ ))}. For compatibility
\item
  \textbf{u\_}: \texttt{ndarray((nactions,\ ))}. For compatibility
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{QLearner}\label{qlearner}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.QLearner()}
\end{Highlighting}
\end{Shaded}

Learns an instrumental control policy through Q-learning

The Q-learning rule is as follows:

\[
\mathbf Q \gets \mathbf Q + \alpha \big(r + \gamma \max_{\mathbf u'} \mathbf u'^\top \mathbf Q \mathbf x' - \mathbf u^\top \mathbf Q \mathbf x \big) \mathbf z,
\]

where \(0 < \alpha < 1\) is the learning rate, \(0 \leq \gamma \leq 1\)
is a discount factor, and where the reward prediction error (RPE) is
\(\delta = (r + \gamma \max_{\mathbf u'} \mathbf u'^\top \mathbf Q \mathbf x' - \mathbf u^\top \mathbf Q \mathbf x)\).
We have also included an eligibility trace \(\mathbf z\) defined as

\[
\mathbf z = \mathbf u \mathbf x^\top +  \gamma \lambda \mathbf z
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{env}: A \texttt{fitr.environments.Graph}
\item
  \textbf{learning\_rate}: Learning rate \(\alpha\)
\item
  \textbf{discount\_factor}: Discount factor \(\gamma\)
\item
  \textbf{trace\_decay}: Eligibility trace decay \(\lambda\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearner.Qmax}\label{qlearner.qmax}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qmax(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Return maximal action value for given state

\[
\max_{u_i}\mathcal Q(\mathbf x, u_i) = \max_{\mathbf u'} \mathbf u'^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of the maximal action value at the given state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearner.Qmean}\label{qlearner.qmean}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qmean(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Return mean action value for given state

\[
Mean \big(\mathcal Q(\mathbf x, :)\big) = \frac{1}{|\mathcal U|} \mathbf 1^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of the maximal action value at the given state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearner.Qx}\label{qlearner.qx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute action values for a given state

\[
\mathcal Q(\mathbf x, :) = \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} vector of values for actions in the given
state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearner.Vx}\label{qlearner.vx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Vx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute value of state \(\mathbf x\)

\[
\mathcal V(\mathbf x) = \mathbf v^\top \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearner.grad\_Qx}\label{qlearner.grad_qx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.grad_Qx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute gradient of action values for a given state

\[
\mathcal Q(\mathbf x, :) = \mathbf Q \mathbf x,
\]

where the gradient is defined as

\[
\frac{\partial}{\partial \mathbf Q} \mathcal Q(\mathbf x, :) = \mathbf 1 \mathbf x^\top,
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} vector of values for actions in the given
state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearner.grad\_Vx}\label{qlearner.grad_vx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.grad_Vx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute the gradient of state value function with respect to parameters
\(\mathbf v\)

\[
\mathcal V(\mathbf x) = \mathbf v^\top \mathbf x,
\]

where the gradient is defined as

\[
\nabla_{\mathbf v} \mathcal V(\mathbf x) = \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearner.grad\_uQx}\label{qlearner.grad_uqx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.grad_uQx(}\VariableTok{self}\NormalTok{, u, x)}
\end{Highlighting}
\end{Shaded}

Compute derivative of value of taking action \(\mathbf u\) in state
\(\mathbf x\) with respect to value function parameters \(\mathbf Q\)

\[
\mathcal Q(\mathbf x, \mathbf u) = \mathbf u^\top \mathbf Q \mathbf x,
\]

where the derivative is defined as

\[
\frac{\partial}{\partial \mathbf Q} \mathcal Q(\mathbf x, \mathbf u) = \mathbf u \mathbf x^\top,
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of action \(\mathbf u\) in state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearner.uQx}\label{qlearner.uqx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.uQx(}\VariableTok{self}\NormalTok{, u, x)}
\end{Highlighting}
\end{Shaded}

Compute value of taking action \(\mathbf u\) in state \(\mathbf x\)

\[
\mathcal Q(\mathbf x, \mathbf u) = \mathbf u^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of action \(\mathbf u\) in state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearner.update}\label{qlearner.update}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.update(}\VariableTok{self}\NormalTok{, x, u, r, x_, u_)}
\end{Highlighting}
\end{Shaded}

Computes value function updates and their derivatives for the Q-learning
model

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{SARSALearner}\label{sarsalearner}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.SARSALearner()}
\end{Highlighting}
\end{Shaded}

Learns an instrumental control policy through the SARSA learning rule

The SARSA learning rule is as follows:

\[
\mathbf Q \gets \mathbf Q + \alpha \big(r + \gamma \mathbf u'^\top \mathbf Q \mathbf x' - \mathbf u^\top \mathbf Q \mathbf x \big) \mathbf z,
\]

where \(0 < \alpha < 1\) is the learning rate, \(0 \leq \gamma \leq 1\)
is a discount factor, and where the reward prediction error (RPE) is
\(\delta = (r + \gamma \mathbf u'^\top \mathbf Q \mathbf x' - \mathbf u^\top \mathbf Q \mathbf x)\).
We have also included an eligibility trace \(\mathbf z\) defined as

\[
\mathbf z = \mathbf u \mathbf x^\top +  \gamma \lambda \mathbf z
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{env}: A \texttt{fitr.environments.Graph}
\item
  \textbf{learning\_rate}: Learning rate \(\alpha\)
\item
  \textbf{discount\_factor}: Discount factor \(\gamma\)
\item
  \textbf{trace\_decay}: Eligibility trace decay \(\lambda\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSALearner.Qmax}\label{sarsalearner.qmax}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qmax(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Return maximal action value for given state

\[
\max_{u_i}\mathcal Q(\mathbf x, u_i) = \max_{\mathbf u'} \mathbf u'^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of the maximal action value at the given state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSALearner.Qmean}\label{sarsalearner.qmean}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qmean(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Return mean action value for given state

\[
Mean \big(\mathcal Q(\mathbf x, :)\big) = \frac{1}{|\mathcal U|} \mathbf 1^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of the maximal action value at the given state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSALearner.Qx}\label{sarsalearner.qx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute action values for a given state

\[
\mathcal Q(\mathbf x, :) = \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} vector of values for actions in the given
state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSALearner.Vx}\label{sarsalearner.vx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Vx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute value of state \(\mathbf x\)

\[
\mathcal V(\mathbf x) = \mathbf v^\top \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSALearner.grad\_Qx}\label{sarsalearner.grad_qx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.grad_Qx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute gradient of action values for a given state

\[
\mathcal Q(\mathbf x, :) = \mathbf Q \mathbf x,
\]

where the gradient is defined as

\[
\frac{\partial}{\partial \mathbf Q} \mathcal Q(\mathbf x, :) = \mathbf 1 \mathbf x^\top,
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} vector of values for actions in the given
state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSALearner.grad\_Vx}\label{sarsalearner.grad_vx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.grad_Vx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute the gradient of state value function with respect to parameters
\(\mathbf v\)

\[
\mathcal V(\mathbf x) = \mathbf v^\top \mathbf x,
\]

where the gradient is defined as

\[
\nabla_{\mathbf v} \mathcal V(\mathbf x) = \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSALearner.grad\_uQx}\label{sarsalearner.grad_uqx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.grad_uQx(}\VariableTok{self}\NormalTok{, u, x)}
\end{Highlighting}
\end{Shaded}

Compute derivative of value of taking action \(\mathbf u\) in state
\(\mathbf x\) with respect to value function parameters \(\mathbf Q\)

\[
\mathcal Q(\mathbf x, \mathbf u) = \mathbf u^\top \mathbf Q \mathbf x,
\]

where the derivative is defined as

\[
\frac{\partial}{\partial \mathbf Q} \mathcal Q(\mathbf x, \mathbf u) = \mathbf u \mathbf x^\top,
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of action \(\mathbf u\) in state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSALearner.uQx}\label{sarsalearner.uqx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.uQx(}\VariableTok{self}\NormalTok{, u, x)}
\end{Highlighting}
\end{Shaded}

Compute value of taking action \(\mathbf u\) in state \(\mathbf x\)

\[
\mathcal Q(\mathbf x, \mathbf u) = \mathbf u^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of action \(\mathbf u\) in state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSALearner.update}\label{sarsalearner.update}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.update(}\VariableTok{self}\NormalTok{, x, u, r, x_, u_)}
\end{Highlighting}
\end{Shaded}

Computes value function updates and their derivatives for the SARSA
model

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Agent}\label{agent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.Agent()}
\end{Highlighting}
\end{Shaded}

Base class for synthetic RL agents.

Arguments:

meta : List of metadata of arbitrary type. e.g.~labels, covariates, etc.
params : List of parameters for the agent. Should be filled for specific
agent.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Agent.action}\label{agent.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Agent.learning}\label{agent.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters and computes gradients

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Agent.reset\_trace}\label{agent.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, state_only}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state\_only}: \texttt{bool}. If the eligibility trace is only
  an \texttt{nstate} dimensional vector (i.e.~for a Pavlovian
  conditioning model) then set to \texttt{True}. For instumental models,
  the eligibility trace should be an \texttt{nactions} by
  \texttt{nstates} matrix, so keep this to \texttt{False} in that case.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{BanditAgent}\label{banditagent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.BanditAgent()}
\end{Highlighting}
\end{Shaded}

A base class for agents in bandit tasks (i.e.~with one step).

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{task}: \texttt{fitr.environments.Graph}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{BanditAgent.action}\label{banditagent.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{BanditAgent.generate\_data}\label{banditagent.generate_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.generate_data(}\VariableTok{self}\NormalTok{, ntrials)}
\end{Highlighting}
\end{Shaded}

For the parent agent, this function generates data from a bandit task

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{ntrials}: \texttt{int} number of trials
\end{itemize}

Returns:

\texttt{fitr.data.BehaviouralData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{BanditAgent.learning}\label{banditagent.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters and computes gradients

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{BanditAgent.log\_prob}\label{banditagent.log_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.log_prob(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Computes the log-likelihood over actions for a given state under the
present agent parameters.

Presently this only works for the state-action value function. In all
other cases, you should define your own log-likelihood function.
However, this can be used as a template.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} log-likelihood vector

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{BanditAgent.reset\_trace}\label{banditagent.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, state_only}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state\_only}: \texttt{bool}. If the eligibility trace is only
  an \texttt{nstate} dimensional vector (i.e.~for a Pavlovian
  conditioning model) then set to \texttt{True}. For instumental models,
  the eligibility trace should be an \texttt{nactions} by
  \texttt{nstates} matrix, so keep this to \texttt{False} in that case.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{MDPAgent}\label{mdpagent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.MDPAgent()}
\end{Highlighting}
\end{Shaded}

A base class for agents that operate on MDPs.

This mainly has implications for generating data.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{task}: \texttt{fitr.environments.Graph}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{MDPAgent.action}\label{mdpagent.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{MDPAgent.generate\_data}\label{mdpagent.generate_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.generate_data(}\VariableTok{self}\NormalTok{, ntrials, state_only}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For the parent agent, this function generates data from a Markov
Decision Process (MDP) task

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{ntrials}: \texttt{int} number of trials
\item
  \textbf{state\_only}: \texttt{bool}. If the eligibility trace is only
  an \texttt{nstate} dimensional vector (i.e.~for a Pavlovian
  conditioning model) then set to \texttt{True}. For instumental models,
  the eligibility trace should be an \texttt{nactions} by
  \texttt{nstates} matrix, so keep this to \texttt{False} in that case.
\end{itemize}

Returns:

\texttt{fitr.data.BehaviouralData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{MDPAgent.learning}\label{mdpagent.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters and computes gradients

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{MDPAgent.reset\_trace}\label{mdpagent.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, state_only}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state\_only}: \texttt{bool}. If the eligibility trace is only
  an \texttt{nstate} dimensional vector (i.e.~for a Pavlovian
  conditioning model) then set to \texttt{True}. For instumental models,
  the eligibility trace should be an \texttt{nactions} by
  \texttt{nstates} matrix, so keep this to \texttt{False} in that case.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{RandomBanditAgent}\label{randombanditagent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.RandomBanditAgent()}
\end{Highlighting}
\end{Shaded}

An agent that simply selects random actions at each trial

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RandomBanditAgent.action}\label{randombanditagent.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RandomBanditAgent.generate\_data}\label{randombanditagent.generate_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.generate_data(}\VariableTok{self}\NormalTok{, ntrials)}
\end{Highlighting}
\end{Shaded}

For the parent agent, this function generates data from a bandit task

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{ntrials}: \texttt{int} number of trials
\end{itemize}

Returns:

\texttt{fitr.data.BehaviouralData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RandomBanditAgent.learning}\label{randombanditagent.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters and computes gradients

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RandomBanditAgent.log\_prob}\label{randombanditagent.log_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.log_prob(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Computes the log-likelihood over actions for a given state under the
present agent parameters.

Presently this only works for the state-action value function. In all
other cases, you should define your own log-likelihood function.
However, this can be used as a template.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} log-likelihood vector

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RandomBanditAgent.reset\_trace}\label{randombanditagent.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, state_only}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state\_only}: \texttt{bool}. If the eligibility trace is only
  an \texttt{nstate} dimensional vector (i.e.~for a Pavlovian
  conditioning model) then set to \texttt{True}. For instumental models,
  the eligibility trace should be an \texttt{nactions} by
  \texttt{nstates} matrix, so keep this to \texttt{False} in that case.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{RandomMDPAgent}\label{randommdpagent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.RandomMDPAgent()}
\end{Highlighting}
\end{Shaded}

An agent that simply selects random actions at each trial

\subsection{Notes}\label{notes}

This has been specified as an \texttt{OnPolicyAgent} arbitrarily.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RandomMDPAgent.action}\label{randommdpagent.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RandomMDPAgent.generate\_data}\label{randommdpagent.generate_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.generate_data(}\VariableTok{self}\NormalTok{, ntrials, state_only}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For the parent agent, this function generates data from a Markov
Decision Process (MDP) task

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{ntrials}: \texttt{int} number of trials
\item
  \textbf{state\_only}: \texttt{bool}. If the eligibility trace is only
  an \texttt{nstate} dimensional vector (i.e.~for a Pavlovian
  conditioning model) then set to \texttt{True}. For instumental models,
  the eligibility trace should be an \texttt{nactions} by
  \texttt{nstates} matrix, so keep this to \texttt{False} in that case.
\end{itemize}

Returns:

\texttt{fitr.data.BehaviouralData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RandomMDPAgent.learning}\label{randommdpagent.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters and computes gradients

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RandomMDPAgent.reset\_trace}\label{randommdpagent.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, state_only}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state\_only}: \texttt{bool}. If the eligibility trace is only
  an \texttt{nstate} dimensional vector (i.e.~for a Pavlovian
  conditioning model) then set to \texttt{True}. For instumental models,
  the eligibility trace should be an \texttt{nactions} by
  \texttt{nstates} matrix, so keep this to \texttt{False} in that case.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{SARSASoftmaxAgent}\label{sarsasoftmaxagent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.SARSASoftmaxAgent()}
\end{Highlighting}
\end{Shaded}

An agent that uses the SARSA learning rule and a softmax policy

The softmax policy selects actions from a multinomial

\[
\mathbf u \sim \mathrm{Multinomial}(1, \mathbf p=\varsigma(\mathbf v)),
\]

whose parameters are

\[
p(\mathbf u|\mathbf v) = \varsigma(\mathbf v) = \frac{e^{\beta \mathbf v}}{\sum_{i}^{|\mathbf v|} e^{\beta v_i}}.
\]

The value function is SARSA:

\[
\mathbf Q \gets \mathbf Q + \alpha \big(r + \gamma \mathbf u'^\top \mathbf Q \mathbf x' - \mathbf u^\top \mathbf Q \mathbf x \big) \mathbf z,
\]

where \(0 < \alpha < 1\) is the learning rate, \(0 \leq \gamma \leq 1\)
is a discount factor, and where the reward prediction error (RPE) is
\(\delta = (r + \gamma \mathbf u'^\top \mathbf Q \mathbf x' - \mathbf u^\top \mathbf Q \mathbf x)\).
We have also included an eligibility trace \(\mathbf z\) defined as

\[
\mathbf z = \mathbf u \mathbf x^\top +  \gamma \lambda \mathbf z
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{task}: \texttt{fitr.environments.Graph}
\item
  \textbf{learning\_rate}: Learning rate \(\alpha\)
\item
  \textbf{discount\_factor}: Discount factor \(\gamma\)
\item
  \textbf{trace\_decay}: Eligibility trace decay \(\lambda\)
\item
  \textbf{inverse\_softmax\_temp}: Inverse softmax temperature \(\beta\)
\item
  \textbf{rng}: \texttt{np.random.RandomState}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSASoftmaxAgent.action}\label{sarsasoftmaxagent.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSASoftmaxAgent.generate\_data}\label{sarsasoftmaxagent.generate_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.generate_data(}\VariableTok{self}\NormalTok{, ntrials, state_only}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For the parent agent, this function generates data from a Markov
Decision Process (MDP) task

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{ntrials}: \texttt{int} number of trials
\item
  \textbf{state\_only}: \texttt{bool}. If the eligibility trace is only
  an \texttt{nstate} dimensional vector (i.e.~for a Pavlovian
  conditioning model) then set to \texttt{True}. For instumental models,
  the eligibility trace should be an \texttt{nactions} by
  \texttt{nstates} matrix, so keep this to \texttt{False} in that case.
\end{itemize}

Returns:

\texttt{fitr.data.BehaviouralData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSASoftmaxAgent.learning}\label{sarsasoftmaxagent.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters and computes gradients

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSASoftmaxAgent.log\_prob}\label{sarsasoftmaxagent.log_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.log_prob(}\VariableTok{self}\NormalTok{, state, action)}
\end{Highlighting}
\end{Shaded}

Computes the log-probability of the given action and state under the
model, while also computing first and second order derivatives.

This model has four free parameters:

\begin{itemize}
\tightlist
\item
  Learning rate \(\alpha\)
\item
  Inverse softmax temperature \(\beta\)
\item
  Discount factor \(\gamma\)
\item
  Trace decay \(\lambda\)
\end{itemize}

\textbf{First-order partial derivatives}

We can break down the computation using the chain rule to reuse
previously computed derivatives:

\[
\pd{\cL}{\alpha}  = \pd{\cL}{\logits} \pd{\logits}{\mathbf q} \pd{\mathbf q}{\mathbf Q} \pd{\mathbf Q}{\alpha}
\]

\[
\pd{\cL}{\beta}   = \pd{\cL}{\logits} \pd{\logits}{\beta}
\]

\[
\pd{\cL}{\gamma}  = \pd{\cL}{\logits} \pd{\logits}{\mathbf q} \pd{\mathbf q}{\mathbf Q} \pd{\mathbf Q}{\gamma}
\]

\[
\pd{\cL}{\lambda} = \pd{\cL}{\logits} \pd{\logits}{\mathbf q} \pd{\mathbf q}{\mathbf Q} \pd{\mathbf Q}{\lambda}
\]

\emph{Action Probabilities}

\[
\partial_\alpha \varsigma = \pd{\varsigma}{\logits} \pd{\logits}{\mathbf q} \pd{\mathbf q}{\mathbf Q} \big( \partial_\alpha \mathbf Q \big) = \beta \big(\partial_{\logits} \varsigma \big)_i \big( \partial_\alpha Q \big)^i_j x^j
\]

\emph{Value Function}

\[
\partial_\alpha Q_{ij} =  \partial_\alpha Q_{ij} + (\delta + \alpha \partial_\alpha \delta) z_{ij}
\]

\[
\partial_\gamma Q_{ij} =  \partial_\gamma Q_{ij} + \alpha \big( (\partial_\gamma \delta) z_{ij} + \delta (\partial_\gamma z_{ij}) \big)
\]

\[
\partial_\lambda Q_{ij} =  \partial_\lambda Q_{ij} + \alpha \big( (\partial_\lambda \delta) z_{ij} + \delta (\partial_\lambda z_{ij}) \big)
\]

\emph{Reward Prediction Error}

\[
\partial_\alpha \delta = (\partial_{\mathbf Q} \delta)_{ij} (\partial_\alpha Q)^{ij}
\]

\[
\partial_\gamma \delta = (\partial_{\mathbf Q} \delta)_{ij} (\partial_\gamma Q)^{ij} + \tilde{u}_i Q^i_j \tilde{x}^j
\]

\[
\partial_\lambda \delta = (\partial_{\mathbf Q} \delta)_{ij} (\partial_\lambda Q)^{ij}
\]

\emph{Trace Decay}

\[
\partial_\gamma z_{ij} = \lambda \big(z_{ij} + \gamma (\partial_\gamma z_{ij}) \big)
\]

\[
\partial_\lambda z_{ij} = \gamma \big(z_{ij} + \lambda (\partial_\lambda z_{ij}) \big)
\]

\emph{Simplified Components of the Gradient Vector}

\[
\pd{\cL}{\alpha}  = \beta \big[\mathbf u - \varsigma(\logits) \big]_i \big( \partial_\alpha Q \big)^i_j x^j   = \beta \big[ u_i (\partial_\alpha Q)^i_j x^j - p(u_i) (\partial_\alpha Q)^i_j x^j \big]
\]

\[
\pd{\cL}{\beta}   =  \big[\mathbf u - \varsigma(\logits)\big]_i Q^i_j x^j = u_i Q^i_j x^j - p(u_i) Q^i_j x^j
\]

\[
\pd{\cL}{\gamma}  = \beta \big[\mathbf u - \varsigma(\logits) \big]_i \big( \partial_\gamma Q \big)^i_j x^j
\]

\[
\pd{\cL}{\lambda} = \beta \big[\mathbf u - \varsigma(\logits) \big]_i \big( \partial_\lambda Q \big)^i_j x^j
\]

\textbf{Second-Order Partial Derivatives}

The Hessian matrix for this model is

\[
\mathbf H = \left[
\begin{array}{cccc}
\pHd{\cL}{\alpha}                  & \pHo{\cL}{\alpha}{\beta}  & \pHo{\cL}{\alpha}{\gamma}  & \pHo{\cL}{\alpha}{\lambda} \\
\pHo{\cL}{\beta}{\alpha}   & \pHd{\cL}{\beta}              & \pHo{\cL}{\beta}{\gamma}   & \pHo{\cL}{\beta}{\lambda}  \\
\pHo{\cL}{\gamma}{\alpha}  & \pHo{\cL}{\gamma}{\beta}  & \pHd{\cL}{\gamma}                  & \pHo{\cL}{\gamma}{\lambda} \\
\pHo{\cL}{\lambda}{\alpha} & \pHo{\cL}{\lambda}{\beta} & \pHo{\cL}{\lambda}{\gamma} & \pHd{\cL}{\lambda}                 \\
\end{array}\right],
\]

where the second-order partial derivatives are such that \(\mathbf H\)
is symmetrical. We must therefore compute 10 second order partial
derivatives, shown below:

\[
\pHd{\cL}{\alpha} = \beta \Big[ (\mathbf u - \varsigma(\logits))_i \big( \partial^2_\alpha Q \big)^i - \big( \partial_\alpha \varsigma \big)_j \big( \partial_\alpha Q)^j_k x^k \Big]_l x^l
\]

\[
\pHd{\cL}{\beta} = \Big( q_i \varsigma(\logits)^i \Big)^2 - \mathbf q \odot \mathbf q \odot \varsigma(\logits)
\]

\[
\pHd{\cL}{\gamma}  = \beta \Big[ (\mathbf u - \varsigma(\logits))_i \big( \partial^2_\gamma Q \big)^i - \big( \partial_\gamma \varsigma \big)_j \big( \partial_\gamma Q)^j_k x^k \Big]_l x^l
\]

\[
\pHd{\cL}{\lambda}  = \beta \Big[ (\mathbf u - \varsigma(\logits))_i \big( \partial^2_\lambda Q \big)^i - \big( \partial_\lambda \varsigma \big)_j \big( \partial_\lambda Q)^j_k x^k \Big]_l x^l
\]

The off diagonal elements of the Hessian are as follows:

\[
\pHo{\cL}{\alpha}{\beta}   = \bigg(\mathbf u - \varsigma(\logits) - \beta \big(\partial_\beta \varsigma \big) \bigg)_i \big( \partial_\alpha Q \big)^i_j x^j
\]

\[
\pHo{\cL}{\beta}{\gamma}   =  \bigg(\mathbf u - \varsigma(\logits) - \beta \big(\partial_\beta \varsigma \big) \bigg)_i \big( \partial_\gamma Q \big)^i_j x^j
\]

\[
\pHo{\cL}{\beta}{\lambda}  =  \bigg(\mathbf u - \varsigma(\logits) - \beta \big(\partial_\beta \varsigma \big) \bigg)_i \big( \partial_\lambda Q \big)^i_j x^j
\]

\[
\pHo{\cL}{\alpha}{\gamma}  = \beta \Big((\mathbf u - \varsigma(\logits))_i \big(\partial_\alpha \partial_\gamma Q \big)^i - \big( \partial_\gamma \varsigma \big)_j \big( \partial_\alpha Q \big)^j \Big)_k x^k
\]

\[
\pHo{\cL}{\alpha}{\lambda} =  \beta \Big((\mathbf u - \varsigma(\logits))_i \big(\partial_\alpha \partial_\lambda Q \big)^i - \big( \partial_\lambda \varsigma \big)_j \big( \partial_\alpha Q \big)^j \Big)_k x^k
\]

\[
\pHo{\cL}{\gamma}{\lambda} =  \beta \Big((\mathbf u - \varsigma(\logits))_i \big(\partial_\lambda \partial_\gamma Q \big)^i - \big( \partial_\lambda \varsigma \big)_j \big( \partial_\gamma Q \big)^j \Big)_k x^k
\]

\emph{Reward Prediction Error}

\[
\partial^2_\alpha \delta = (\partial_{\mathbf Q} \delta)_{ij} (\partial^2_\alpha Q)^{ij}
\]

\[
\partial^2_\gamma \delta = (\partial_{\mathbf Q} \delta)_{ij} (\partial^2_\gamma Q)^{ij} + 2 \tilde{u}_i \big(\partial_\gamma Q\big)^i_j \tilde{x}^j
\]

\[
\partial^2_\lambda \delta = (\partial_{\mathbf Q} \delta)_{ij} (\partial^2_\lambda Q)^{ij}
\]

\[
\partial_\alpha \partial_\gamma \delta = (\partial_{\mathbf Q} \delta)_{ij} (\partial_\gamma \partial_\alpha Q)^{ij} + \tilde{u}_i \big(\partial_\alpha Q\big)^i_j \tilde{x}^j
\]

\[
\partial_\alpha \partial_\lambda \delta = (\partial_{\mathbf Q} \delta)_{ij} (\partial_\alpha \partial_\lambda Q)^{ij}
\]

\[
\partial_\gamma \partial_\lambda \delta = (\partial_{\mathbf Q} \delta)_{ij} (\partial_\gamma \partial_\lambda Q)^{ij} + \tilde{u}_i \big(\partial_\lambda Q\big)^i_j \tilde{x}^j
\]

\emph{Value Function}

\[
\partial^2_\alpha Q_{ij} = \partial^2_\alpha Q_{ij} + 2(\partial_\alpha \delta) z_{ij} + \alpha (\partial^2_\alpha \delta) z_{ij}
\]

\[
\partial^2_\gamma Q_{ij} = \partial^2_\gamma Q_{ij} +  \alpha \Big( \big(\partial^2_\gamma \delta \big)z_{ij} +  \big(\partial_\gamma \delta \big) \big(\partial_\gamma z_{ij}\big) + \big(\partial_\gamma \delta \big) \big(\partial^2_\gamma z_{ij}\big) \Big)
\]

\[
\partial^2_\lambda Q_{ij} = \partial^2_\lambda Q_{ij} +  \alpha \Big( \big(\partial^2_\lambda \delta \big)z_{ij} +  \big(\partial_\lambda \delta \big) \big(\partial_\lambda z_{ij}\big) + \big(\partial_\lambda \delta \big) \big(\partial^2_\lambda z_{ij}\big) \Big)
\]

\[
\partial_\alpha \partial_\gamma Q_{ij} =  \partial_\alpha \partial_\gamma Q_{ij} + (\partial_\gamma \delta) z_{ij} + \delta \big(\partial_\gamma z_{ij} \big) + \alpha(\partial_\alpha \delta)\big(\partial_\gamma z_{ij} \big) + \alpha(\partial_\alpha \partial_\gamma \delta) z_{ij}
\]

\[
\partial_\alpha \partial_\lambda Q_{ij} = \partial_\alpha \partial_\lambda Q_{ij} + (\partial_\lambda \delta) z_{ij} + \delta \big(\partial_\lambda z_{ij} \big) + \alpha(\partial_\alpha \delta)\big(\partial_\lambda z_{ij} \big) + \alpha(\partial_\alpha \partial_\lambda \delta) z_{ij}
\]

\[
\partial_\gamma \partial_\lambda Q_{ij} = \partial_\gamma \partial_\lambda Q_{ij} + \alpha \Big[ \big( \partial_\lambda \partial_\gamma \delta \big) z_{ij} + \big( \partial_\gamma \delta \big)\big(\partial_\lambda z_{ij} \big) + \big(\partial_\lambda \delta \big)\big(\partial_\gamma z_{ij} \big) + \delta \big(\partial_\lambda \partial_\gamma z_{ij} \big) \Big]
\]

\emph{Trace Decay}

\[
\partial^2_\gamma z = \lambda \Big( 2\big(\partial_\gamma z\big) + \gamma \big(\partial^2_\gamma z \big) \Big)
\]

\[
\partial^2_\lambda z = \gamma \Big( 2\big(\partial_\lambda z\big) + \lambda \big(\partial^2_\lambda z \big) \Big)
\]

\[
\partial_\gamma \partial_\lambda z = z  + \gamma \big( \partial_\gamma z \big) + \lambda \big( \partial_\lambda z \big) + \lambda \gamma \big( \partial_\gamma \partial_\lambda z \big)
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{action}: \texttt{ndarray(nactions)}. One-hot action vector
\item
  \textbf{state}: \texttt{ndarray(nstates)}. One-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSASoftmaxAgent.reset\_trace}\label{sarsasoftmaxagent.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, state_only}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state\_only}: \texttt{bool}. If the eligibility trace is only
  an \texttt{nstate} dimensional vector (i.e.~for a Pavlovian
  conditioning model) then set to \texttt{True}. For instumental models,
  the eligibility trace should be an \texttt{nactions} by
  \texttt{nstates} matrix, so keep this to \texttt{False} in that case.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{SARSAStickySoftmaxAgent}\label{sarsastickysoftmaxagent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.SARSAStickySoftmaxAgent()}
\end{Highlighting}
\end{Shaded}

An agent that uses the SARSA learning rule and a sticky softmax policy

The sticky softmax policy selects actions from a multinomial

\[
\mathbf u \sim \mathrm{Multinomial}(1, \mathbf p=\varsigma(\mathbf v)),
\]

whose parameters are

\[
p(\mathbf u|\mathbf v, \mathbf u_{t-1}) = \varsigma(\mathbf v, \mathbf u_{t-1}) = \frac{e^{\beta \mathbf v + \beta^\rho \mathbf u_{t-1}}}{\sum_{i}^{|\mathbf v|} e^{\beta v_i + \beta^\rho u_{t-1}^{(i)}}}.
\]

The value function is SARSA:

\[
\mathbf Q \gets \mathbf Q + \alpha \big(r + \gamma \mathbf u'^\top \mathbf Q \mathbf x' - \mathbf u^\top \mathbf Q \mathbf x \big) \mathbf z,
\]

where \(0 < \alpha < 1\) is the learning rate, \(0 \leq \gamma \leq 1\)
is a discount factor, and where the reward prediction error (RPE) is
\(\delta = (r + \gamma \mathbf u'^\top \mathbf Q \mathbf x' - \mathbf u^\top \mathbf Q \mathbf x)\).
We have also included an eligibility trace \(\mathbf z\) defined as

\[
\mathbf z = \mathbf u \mathbf x^\top +  \gamma \lambda \mathbf z
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{task}: \texttt{fitr.environments.Graph}
\item
  \textbf{learning\_rate}: Learning rate \(\alpha\)
\item
  \textbf{discount\_factor}: Discount factor \(\gamma\)
\item
  \textbf{trace\_decay}: Eligibility trace decay \(\lambda\)
\item
  \textbf{inverse\_softmax\_temp}: Inverse softmax temperature \(\beta\)
\item
  \textbf{perseveration}: Perseveration parameter \(\beta^\rho\)
\item
  \textbf{rng}: \texttt{np.random.RandomState}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSAStickySoftmaxAgent.action}\label{sarsastickysoftmaxagent.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSAStickySoftmaxAgent.generate\_data}\label{sarsastickysoftmaxagent.generate_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.generate_data(}\VariableTok{self}\NormalTok{, ntrials, state_only}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For the parent agent, this function generates data from a Markov
Decision Process (MDP) task

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{ntrials}: \texttt{int} number of trials
\item
  \textbf{state\_only}: \texttt{bool}. If the eligibility trace is only
  an \texttt{nstate} dimensional vector (i.e.~for a Pavlovian
  conditioning model) then set to \texttt{True}. For instumental models,
  the eligibility trace should be an \texttt{nactions} by
  \texttt{nstates} matrix, so keep this to \texttt{False} in that case.
\end{itemize}

Returns:

\texttt{fitr.data.BehaviouralData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSAStickySoftmaxAgent.learning}\label{sarsastickysoftmaxagent.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters and computes gradients

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSAStickySoftmaxAgent.reset\_trace}\label{sarsastickysoftmaxagent.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, state_only}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state\_only}: \texttt{bool}. If the eligibility trace is only
  an \texttt{nstate} dimensional vector (i.e.~for a Pavlovian
  conditioning model) then set to \texttt{True}. For instumental models,
  the eligibility trace should be an \texttt{nactions} by
  \texttt{nstates} matrix, so keep this to \texttt{False} in that case.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{QLearningSoftmaxAgent}\label{qlearningsoftmaxagent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.QLearningSoftmaxAgent()}
\end{Highlighting}
\end{Shaded}

An agent that uses the Q-learning rule and a softmax policy

The softmax policy selects actions from a multinomial

\[
\mathbf u \sim \mathrm{Multinomial}(1, \mathbf p=\varsigma(\mathbf v)),
\]

whose parameters are

\[
p(\mathbf u|\mathbf v) = \varsigma(\mathbf v) = \frac{e^{\beta \mathbf v}}{\sum_{i}^{|\mathbf v|} e^{\beta v_i}}.
\]

The value function is Q-learning:

\[
\mathbf Q \gets \mathbf Q + \alpha \big(r + \gamma \max_{\mathbf u'} \mathbf u'^\top \mathbf Q \mathbf x' - \mathbf u^\top \mathbf Q \mathbf x \big) \mathbf z,
\]

where \(0 < \alpha < 1\) is the learning rate, \(0 \leq \gamma \leq 1\)
is a discount factor, and where the reward prediction error (RPE) is
\(\delta = (r + \gamma \max_{\mathbf u'} \mathbf u'^\top \mathbf Q \mathbf x' - \mathbf u^\top \mathbf Q \mathbf x)\).
The eligibility trace \(\mathbf z\) is defined as

\[
\mathbf z = \mathbf u \mathbf x^\top +  \gamma \lambda \mathbf z
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{task}: \texttt{fitr.environments.Graph}
\item
  \textbf{learning\_rate}: Learning rate \(\alpha\)
\item
  \textbf{discount\_factor}: Discount factor \(\gamma\)
\item
  \textbf{trace\_decay}: Eligibility trace decay \(\lambda\)
\item
  \textbf{inverse\_softmax\_temp}: Inverse softmax temperature \(\beta\)
\item
  \textbf{rng}: \texttt{np.random.RandomState}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearningSoftmaxAgent.action}\label{qlearningsoftmaxagent.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearningSoftmaxAgent.generate\_data}\label{qlearningsoftmaxagent.generate_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.generate_data(}\VariableTok{self}\NormalTok{, ntrials, state_only}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For the parent agent, this function generates data from a Markov
Decision Process (MDP) task

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{ntrials}: \texttt{int} number of trials
\item
  \textbf{state\_only}: \texttt{bool}. If the eligibility trace is only
  an \texttt{nstate} dimensional vector (i.e.~for a Pavlovian
  conditioning model) then set to \texttt{True}. For instumental models,
  the eligibility trace should be an \texttt{nactions} by
  \texttt{nstates} matrix, so keep this to \texttt{False} in that case.
\end{itemize}

Returns:

\texttt{fitr.data.BehaviouralData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearningSoftmaxAgent.learning}\label{qlearningsoftmaxagent.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters and computes gradients

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearningSoftmaxAgent.reset\_trace}\label{qlearningsoftmaxagent.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, state_only}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state\_only}: \texttt{bool}. If the eligibility trace is only
  an \texttt{nstate} dimensional vector (i.e.~for a Pavlovian
  conditioning model) then set to \texttt{True}. For instumental models,
  the eligibility trace should be an \texttt{nactions} by
  \texttt{nstates} matrix, so keep this to \texttt{False} in that case.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{RWSoftmaxAgent}\label{rwsoftmaxagent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.RWSoftmaxAgent()}
\end{Highlighting}
\end{Shaded}

An instrumental Rescorla-Wagner agent with a softmax policy

The softmax policy selects actions from a multinomial

\[
\mathbf u \sim \mathrm{Multinomial}(1, \mathbf p=\varsigma(\mathbf v)),
\]

whose parameters are

\[
p(\mathbf u|\mathbf v) = \varsigma(\mathbf v) = \frac{e^{\beta \mathbf v}}{\sum_{i}^{|\mathbf v|} e^{\beta v_i}}.
\]

The value function is the Rescorla-Wagner learning rule:

\[
\mathbf Q \gets \mathbf Q + \alpha \big(r - \mathbf u^\top \mathbf Q \mathbf x \big) \mathbf u \mathbf x^\top,
\]

where \(0 < \alpha < 1\) is the learning rate, \(0 \leq \gamma \leq 1\)
is a discount factor, and where the reward prediction error (RPE) is
\(\delta = (r - \mathbf u^\top \mathbf Q \mathbf x)\).

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{task}: \texttt{fitr.environments.Graph}
\item
  \textbf{learning\_rate}: Learning rate \(\alpha\)
\item
  \textbf{inverse\_softmax\_temp}: Inverse softmax temperature \(\beta\)
\item
  \textbf{rng}: \texttt{np.random.RandomState}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWSoftmaxAgent.action}\label{rwsoftmaxagent.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWSoftmaxAgent.generate\_data}\label{rwsoftmaxagent.generate_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.generate_data(}\VariableTok{self}\NormalTok{, ntrials)}
\end{Highlighting}
\end{Shaded}

For the parent agent, this function generates data from a bandit task

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{ntrials}: \texttt{int} number of trials
\end{itemize}

Returns:

\texttt{fitr.data.BehaviouralData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWSoftmaxAgent.learning}\label{rwsoftmaxagent.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters and computes gradients

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWSoftmaxAgent.log\_prob}\label{rwsoftmaxagent.log_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.log_prob(}\VariableTok{self}\NormalTok{, state, action)}
\end{Highlighting}
\end{Shaded}

Computes the log-probability of an action taken by the agent in a given
state, as well as updates all partial derivatives with respect to the
parameters.

This function overrides the \texttt{log\_prob} method of the parent
class.

Let

\begin{itemize}
\tightlist
\item
  \(n_u \in \mathbb N_+\) be the dimensionality of the action space
\item
  \(n_x \in \mathbb N_+\) be the dimensionality of the state space
\item
  \(\mathbf u = (u_0, u_1, u_{n_u})^\top\) be a one-hot action vector
\item
  \(\mathbf x = (x_0, x_1, x_{n_x})^\top\) be a one-hot action vector
\item
  \(\mathbf Q \in \mathbb R^{n_u \times n_x}\) be the state-action value
  function parameters
\item
  \(\beta \in \mathbb R\) be the inverse softmax temperature
\item
  \(\alpha \in [0, 1]\) be the learning rate
\item
  \(\varsigma(\boldsymbol\pi) = p(\mathbf u | \mathbf Q, \beta)\) be a
  softmax function with logits \(\pi_i = \beta Q_{ij} x^j\) (shown in
  Einstein summation convention).
\item
  \(\mathcal L = \log p(\mathbf u | \mathbf Q, \beta)\) be the
  log-likelihood function for trial \(t\)
\item
  \(q_i = Q_{ij} x^j\) be the value of the state \(x^j\)
\item
  \(v^i = e^{\beta q_i}\) be the softmax potential
\item
  \(\eta(\boldsymbol\pi)\) be the softmax partition function.
\end{itemize}

Then we have the partial derivative of \(\mathcal L\) at trial \(t\)
with respect to \(\alpha\)

\[
\partial_{\alpha} \mathcal L = \beta \Big[ \big(\mathbf u - \varsigma(\pi)\big)_i (\partial_{\alpha} Q)^i_j x^j \Big],
\]

and with respect to \(\beta\)

\[
\partial_{\beta} \mathcal L = u_i \Big(\mathbf I_{n_u \times n_u} - \varsigma(\boldsymbol\pi)\Big)^i_j Q_{jk} x^k.
\]

We also compute the Hessian \(\mathbf H\), defined as

\[
\mathbf H = \left[
\begin{array}{cc}
\partial^2_{\alpha} \mathcal L & \partial_{\alpha} \partial_{\beta} \mathcal L \\
\partial_{\beta} \partial_{\alpha} \mathcal L & \partial^2_{\beta} \mathcal L \\
\end{array}\right].
\]

The components of \(\mathbf H\) are

\[
\partial^2_{\alpha} \mathcal L = \beta \Big( (\mathbf u - \varsigma(\boldsymbol\pi))_i (\partial^2_\alpha \mathbf Q)^i - \partial_{\alpha} \varsigma(\boldsymbol\pi)_i (\partial_{\alpha} \mathbf Q)^i \Big)_j x^j,
\]

\[
\partial^2_{\beta} \mathcal L = u_i \Big( \Big),
\]

\[
\partial_{\alpha} \partial_{\beta} \mathcal L = \Bigg[ (u - \varsigma(\boldsymbol\pi)) - \beta \partial_{\beta} \varsigma(\boldsymbol\pi) \Bigg]_i (\partial_{\alpha} Q)^i_k x^k.
\]

and where
\(\partial_{\beta} \partial_{\alpha} \mathcal L = \partial_{\alpha} \partial_{\beta} \mathcal L\)
since the second derivatives of \(\mathcal L\) are continuous in the
neighbourhood of the parameters.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{action}: \texttt{ndarray(nactions)}. One-hot action vector
\item
  \textbf{state}: \texttt{ndarray(nstates)}. One-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWSoftmaxAgent.reset\_trace}\label{rwsoftmaxagent.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, state_only}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state\_only}: \texttt{bool}. If the eligibility trace is only
  an \texttt{nstate} dimensional vector (i.e.~for a Pavlovian
  conditioning model) then set to \texttt{True}. For instumental models,
  the eligibility trace should be an \texttt{nactions} by
  \texttt{nstates} matrix, so keep this to \texttt{False} in that case.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{RWStickySoftmaxAgent}\label{rwstickysoftmaxagent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.RWStickySoftmaxAgent()}
\end{Highlighting}
\end{Shaded}

An instrumental Rescorla-Wagner agent with a `sticky' softmax policy

The softmax policy selects actions from a multinomial

\[
\mathbf u \sim \mathrm{Multinomial}(1, \mathbf p=\varsigma(\mathbf v, \mathbf u_{t-1})).
\]

whose parameters are

\[
p(\mathbf u|\mathbf v, \mathbf u_{t-1}) = \varsigma(\mathbf v, \mathbf u_{t-1}) = \frac{e^{\beta \mathbf v + \beta^\rho \mathbf u_{t-1}}}{\sum_{i}^{|\mathbf v|} e^{\beta v_i + \beta^\rho u_{t-1}^{(i)}}}.
\]

The value function is the Rescorla-Wagner learning rule:

\[
\mathbf Q \gets \mathbf Q + \alpha \big(r - \mathbf u^\top \mathbf Q \mathbf x \big) \mathbf u \mathbf x^\top,
\]

where \(0 < \alpha < 1\) is the learning rate, \(0 \leq \gamma \leq 1\)
is a discount factor, and where the reward prediction error (RPE) is
\(\delta = (r - \mathbf u^\top \mathbf Q \mathbf x)\).

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{task}: \texttt{fitr.environments.Graph}
\item
  \textbf{learning\_rate}: Learning rate \(\alpha\)
\item
  \textbf{inverse\_softmax\_temp}: Inverse softmax temperature \(\beta\)
\item
  \textbf{perseveration}: Perseveration parameter \(\beta^ ho\)
\item
  \textbf{rng}: \texttt{np.random.RandomState}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWStickySoftmaxAgent.action}\label{rwstickysoftmaxagent.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWStickySoftmaxAgent.generate\_data}\label{rwstickysoftmaxagent.generate_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.generate_data(}\VariableTok{self}\NormalTok{, ntrials)}
\end{Highlighting}
\end{Shaded}

For the parent agent, this function generates data from a bandit task

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{ntrials}: \texttt{int} number of trials
\end{itemize}

Returns:

\texttt{fitr.data.BehaviouralData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWStickySoftmaxAgent.learning}\label{rwstickysoftmaxagent.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters and computes gradients

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWStickySoftmaxAgent.log\_prob}\label{rwstickysoftmaxagent.log_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.log_prob(}\VariableTok{self}\NormalTok{, state, action)}
\end{Highlighting}
\end{Shaded}

Computes the log-probability of an action taken by the agent in a given
state, as well as updates all partial derivatives with respect to the
parameters.

This function overrides the \texttt{log\_prob} method of the parent
class.

Let

\begin{itemize}
\tightlist
\item
  \(n_u \in \mathbb N_+\) be the dimensionality of the action space
\item
  \(n_x \in \mathbb N_+\) be the dimensionality of the state space
\item
  \(\mathbf u = (u_0, u_1, u_{n_u})^\top\) be a one-hot action vector
\item
  \(\tilde{\mathbf u}\) be a one-hot vector representing the last
  trial's action, where at trial 0, \(\tilde{\mathbf u} = \mathbf 0\).
\item
  \(\mathbf x = (x_0, x_1, x_{n_x})^\top\) be a one-hot action vector
\item
  \(\mathbf Q \in \mathbb R^{n_u \times n_x}\) be the state-action value
  function parameters
\item
  \(\beta \in \mathbb R\) be the inverse softmax temperature scaling the
  action values
\item
  \(\rho \in \mathbb R\) be the inverse softmax temperature scaling the
  influence of the past trial's action
\item
  \(\alpha \in [0, 1]\) be the learning rate
\item
  \(\varsigma(\boldsymbol\pi) = p(\mathbf u | \mathbf Q, \beta, \rho)\)
  be a softmax function with logits
  \(\pi_i = \beta Q_{ij} x^j + \rho \tilde{u}_i\) (shown in Einstein
  summation convention).
\item
  \(\mathcal L = \log p(\mathbf u | \mathbf Q, \beta, \rho)\) be the
  log-likelihood function for trial \(t\)
\item
  \(q_i = Q_{ij} x^j\) be the value of the state \(x^j\)
\item
  \(v^i = e^{\beta q_i + \rho \tilde{u}_i}\) be the softmax potential
\item
  \(\eta(\boldsymbol\pi)\) be the softmax partition function.
\end{itemize}

Then we have the partial derivative of \(\mathcal L\) at trial \(t\)
with respect to \(\alpha\)

\[
\partial_{\alpha} \mathcal L = \beta \Big[ \big(\mathbf u - \varsigma(\pi)\big)_i (\partial_{\alpha} Q)^i_j x^j \Big],
\]

and with respect to \(\beta\)

\[
\partial_{\beta} \mathcal L = u_i \Big(\mathbf I_{n_u \times n_u} - \varsigma(\boldsymbol\pi)\Big)^i_j Q_{jk} x^k
\]

and with respect to \(\rho\)

\[
\partial_{\rho} \mathcal L = u_i \Big(\mathbf I_{n_u \times n_u} - \varsigma(\boldsymbol\pi)\Big)^i_j \tilde{u}^j.
\]

We also compute the Hessian \(\mathbf H\), defined as

\[
\mathbf H = \left[
\begin{array}{ccc}
\partial^2_{\alpha} \mathcal L & \partial_{\alpha} \partial_{\beta} \mathcal L & \partial_{\alpha} \partial_{\rho} \mathcal L \\
\partial_{\beta} \partial_{\alpha} \mathcal L & \partial^2_{\beta} \mathcal L & \partial_{\beta} \partial_{\rho} \mathcal L \\
\partial_{\rho} \partial_{\alpha} \mathcal L & \partial_{\rho} \partial_{\beta} \mathcal L & \partial^2_{\rho} \mathcal L \\
\end{array}\right].
\]

The components of \(\mathbf H\) are virtually identical to that of
\texttt{RWSoftmaxAgent}, with the exception of the
\(\partial_{\rho} \partial_{\alpha} \mathcal L\) and
\(\partial_{\beta} \partial_{\rho} \mathcal L\)

\[
\partial^2_{\alpha} \mathcal L = \beta \Big( (\mathbf u - \varsigma(\boldsymbol\pi))_i (\partial^2_{\alpha} \mathbf Q)^i - \partial_{\alpha} \varsigma(\boldsymbol\pi)_i (\partial_{\alpha} \mathbf Q)^i \Big)_j x^j,
\]

\[
\partial^2_{\beta} \mathcal L = u_k \Bigg(\frac{(q_i q_i v^i v^i}{z^2} - \frac{q_i q_i v^i}{z} \Bigg)^k
\]

\[
\partial_{\alpha} \partial_{\beta} \mathcal L = \Bigg[ (u - \varsigma(\boldsymbol\pi)) - \beta \partial_{\beta} \varsigma(\boldsymbol\pi) \Bigg]_i (\partial_{\alpha} Q)^i_k x^k
\]

\[
\partial_{\alpha} \partial_{\rho} \mathcal L = - \beta \Big( \partial_{\boldsymbol\pi} \varsigma(\boldsymbol\pi)_i \tilde{u}^i \Big)_j (\partial_{\alpha} Q)^j_k x^k
\]

and where \(\mathbf H\) is symmetric since the second derivatives of
\(\mathcal L\) are continuous in the neighbourhood of the parameters.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{action}: \texttt{ndarray(nactions)}. One-hot action vector
\item
  \textbf{state}: \texttt{ndarray(nstates)}. One-hot state vector
\end{itemize}

Returns:

\texttt{float}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWStickySoftmaxAgent.reset\_trace}\label{rwstickysoftmaxagent.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, state_only}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state\_only}: \texttt{bool}. If the eligibility trace is only
  an \texttt{nstate} dimensional vector (i.e.~for a Pavlovian
  conditioning model) then set to \texttt{True}. For instumental models,
  the eligibility trace should be an \texttt{nactions} by
  \texttt{nstates} matrix, so keep this to \texttt{False} in that case.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{RWSoftmaxAgentRewardSensitivity}\label{rwsoftmaxagentrewardsensitivity}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.RWSoftmaxAgentRewardSensitivity()}
\end{Highlighting}
\end{Shaded}

An instrumental Rescorla-Wagner agent with a softmax policy, whose
experienced reward is scaled by a factor \(\rho\).

The softmax policy selects actions from a multinomial

\[
\mathbf u \sim \mathrm{Multinomial}(1, \mathbf p=\varsigma(\mathbf v)),
\]

whose parameters are

\[
p(\mathbf u|\mathbf v) = \varsigma(\mathbf v) = \frac{e^{\beta \mathbf v}}{\sum_{i}^{|\mathbf v|} e^{\beta v_i}}.
\]

The value function is the Rescorla-Wagner learning rule with scaled
reward \(\rho r\):

\[
\mathbf Q \gets \mathbf Q + \alpha \big(\rho r - \mathbf u^\top \mathbf Q \mathbf x \big) \mathbf u \mathbf x^\top,
\]

where \(0 < \alpha < 1\) is the learning rate, \(0 \leq \gamma \leq 1\)
is a discount factor, and where the reward prediction error (RPE) is
\(\delta = (\rho r - \mathbf u^\top \mathbf Q \mathbf x)\).

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{task}: \texttt{fitr.environments.Graph}
\item
  \textbf{learning\_rate}: Learning rate \(\alpha\)
\item
  \textbf{inverse\_softmax\_temp}: Inverse softmax temperature \(\beta\)
\item
  \textbf{reward\_sensitivity}: Reward sensitivity parameter \(\rho\)
\item
  \textbf{rng}: \texttt{np.random.RandomState}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWSoftmaxAgentRewardSensitivity.action}\label{rwsoftmaxagentrewardsensitivity.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWSoftmaxAgentRewardSensitivity.generate\_data}\label{rwsoftmaxagentrewardsensitivity.generate_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.generate_data(}\VariableTok{self}\NormalTok{, ntrials)}
\end{Highlighting}
\end{Shaded}

For the parent agent, this function generates data from a bandit task

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{ntrials}: \texttt{int} number of trials
\end{itemize}

Returns:

\texttt{fitr.data.BehaviouralData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWSoftmaxAgentRewardSensitivity.learning}\label{rwsoftmaxagentrewardsensitivity.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters and computes gradients

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWSoftmaxAgentRewardSensitivity.log\_prob}\label{rwsoftmaxagentrewardsensitivity.log_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.log_prob(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Computes the log-likelihood over actions for a given state under the
present agent parameters.

Presently this only works for the state-action value function. In all
other cases, you should define your own log-likelihood function.
However, this can be used as a template.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} log-likelihood vector

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWSoftmaxAgentRewardSensitivity.reset\_trace}\label{rwsoftmaxagentrewardsensitivity.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, state_only}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state\_only}: \texttt{bool}. If the eligibility trace is only
  an \texttt{nstate} dimensional vector (i.e.~for a Pavlovian
  conditioning model) then set to \texttt{True}. For instumental models,
  the eligibility trace should be an \texttt{nactions} by
  \texttt{nstates} matrix, so keep this to \texttt{False} in that case.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}
